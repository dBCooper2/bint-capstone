{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v4 works so v5 is a cleaned up version\n",
    "\n",
    "\n",
    "To get around the timeout errors we faced in prior scraping attempts, this approach will continually write results to a pkl file as the scraping progresses. This way, any error will not invalidate the progress made + allows for a starting point for the reattempt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping games to a pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting unique game_ids \n",
    "\n",
    "from the shots dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('all-shots.pkl') \n",
    "df = df.to_pandas(use_pyarrow_extension_array=True)\n",
    "print(df['GAME_ID'].nunique())\n",
    "game_ids = df['GAME_ID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continual scraping \n",
    "featuring\n",
    "- pkl cache\n",
    "- tqdm + logging\n",
    "- exponential retries\n",
    "- manual timeout waits\n",
    "- ability to start where last attempt stopped (hasnt crashed so this is untested)\n",
    "\n",
    "\n",
    "Note that the 'cache' is basically a dictionary with game_ids as keys and endpoint response jsons as values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load/save the pkl cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_cache(cache_path: str) -> Dict[str, dict]:\n",
    "    \"\"\"Load existing cache or create new one if it doesn't exist.\"\"\"\n",
    "    if Path(cache_path).exists():\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cache(cache: Dict[str, dict], cache_path: str) -> None:\n",
    "    \"\"\"Save the cache to disk.\"\"\"\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump(cache, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scrape for a single game row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_game_level_row(game_id: str, timeout: int = 30, max_retries: int = 3) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Fetch game stats from NBA API with timeout handling and automatic retries.\n",
    "    \n",
    "    Args:\n",
    "        game_id: NBA game identifier\n",
    "        timeout: Request timeout in seconds\n",
    "        max_retries: Maximum number of retry attempts for timeout errors\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Host\": \"stats.nba.com\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0\",\n",
    "        \"Accept\": \"application/json, text/plain, */*\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"x-nba-stats-origin\": \"stats\",\n",
    "        \"x-nba-stats-token\": \"true\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Referer\": \"https://stats.nba.com/\",\n",
    "        \"Pragma\": \"no-cache\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "    }\n",
    "    \n",
    "    url = f'https://stats.nba.com/stats/boxscoretraditionalv3?EndPeriod=0&EndRange=0&GameID={'00'+str(game_id)}&RangeType=0&StartPeriod=0&StartRange=0'\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "            \n",
    "        except Timeout:\n",
    "            wait_time = (attempt + 1) * 5 + random.uniform(1, 3)  # Exponential backoff with jitter\n",
    "            logging.warning(f\"Timeout for game {game_id} (attempt {attempt + 1}/{max_retries}). \"\n",
    "                          f\"Waiting {wait_time:.1f} seconds before retry...\")\n",
    "            time.sleep(wait_time)\n",
    "            \n",
    "        except RequestException as e:\n",
    "            logging.error(f\"Error fetching game {game_id}: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "    logging.error(f\"Max retries ({max_retries}) reached for game {game_id}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scrape for all games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_game_stats(game_ids: List[str], cache_path: str = 'nba_games_cache.pkl', \n",
    "                     delay: float = 1.0, save_frequency: int = 10,\n",
    "                     timeout: int = 30, max_retries: int = 3) -> Dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Scrape game stats with progress bar, timeout handling, and automatic retries.\n",
    "    \n",
    "    Args:\n",
    "        game_ids: List of NBA game IDs to scrape\n",
    "        cache_path: Path to save/load the pickle cache file\n",
    "        delay: Time to wait between requests in seconds\n",
    "        save_frequency: How often to save the cache (every N successful requests)\n",
    "        timeout: Request timeout in seconds\n",
    "        max_retries: Maximum number of retry attempts for timeout errors\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping game IDs to their stats data\n",
    "    \"\"\"\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    # Load existing cache\n",
    "    cache = load_or_create_cache(cache_path)\n",
    "    logging.info(f\"Loaded cache with {len(cache)} existing games\")\n",
    "    \n",
    "    # Filter out already cached games\n",
    "    games_to_scrape = [gid for gid in game_ids if gid not in cache]\n",
    "    logging.info(f\"Found {len(games_to_scrape)} new games to scrape\")\n",
    "    \n",
    "    successful_requests = 0\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(games_to_scrape, desc=\"Scraping games\", unit=\"game\")\n",
    "    \n",
    "    for game_id in pbar:\n",
    "        try:\n",
    "            # Update progress bar description with current game\n",
    "            pbar.set_description(f\"Scraping game {game_id}\")\n",
    "            \n",
    "            # Fetch game data with timeout handling and retries\n",
    "            game_data = create_game_level_row(game_id, timeout=timeout, max_retries=max_retries)\n",
    "            \n",
    "            if game_data is not None:\n",
    "                cache[game_id] = game_data\n",
    "                successful_requests += 1\n",
    "                \n",
    "                # Update progress bar postfix with success count\n",
    "                pbar.set_postfix(\n",
    "                    successful=successful_requests,\n",
    "                    cached_total=len(cache)\n",
    "                )\n",
    "                \n",
    "                # Save periodically\n",
    "                if successful_requests % save_frequency == 0:\n",
    "                    save_cache(cache, cache_path)\n",
    "                    logging.info(f\"Saved cache with {len(cache)} games\")\n",
    "            \n",
    "            # Wait between requests\n",
    "            time.sleep(delay)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error processing game {game_id}: {str(e)}\")\n",
    "            # Save cache on error to preserve progress\n",
    "            save_cache(cache, cache_path)\n",
    "            logging.info(\"Saved cache due to error\")\n",
    "            raise\n",
    "    \n",
    "    # Close progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    # Final save\n",
    "    save_cache(cache, cache_path)\n",
    "    logging.info(f\"Scraping completed. Final cache contains {len(cache)} games\")\n",
    "    \n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main()\n",
    "\n",
    "Note: 1s delay may be overkill, but since my attempts did not crash I did not attempt to modify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    game_stats_2024 = scrape_game_stats(\n",
    "        game_ids=game_ids,\n",
    "        cache_path='nba_games_cache.pkl',\n",
    "        delay=1.0,  # 1 second delay between requests\n",
    "        save_frequency=10,  # Save every 10 successful requests\n",
    "        timeout=30,  # 30 second timeout\n",
    "        max_retries=3  # Retry up to 3 times on timeout\n",
    "    )\n",
    "except Exception as e:\n",
    "    logging.error(f\"Scraping stopped due to error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing pkl cache into dataset df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load pkl into cache dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nba_games_cache.pkl', 'rb') as f:\n",
    "    cache = pickle.load(f)\n",
    "#print(f'number of game ids: {len(game_ids)}\\nnumber of scraped games: {len(cache)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterate through cache using `pd.json_normalize()` to create a row for each game id's value\n",
    "\n",
    "Note: I had a more complicated and verbose version of this wrapped in a function, but while it worked for 2024 sample cache, it didnt for the entire game cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_games = []\n",
    "num_skipped = 0\n",
    "for game_id, game_data in cache.items():\n",
    "    try:\n",
    "        assert game_data is not None\n",
    "        df = pd.json_normalize(game_data)\n",
    "        all_games.append(df)\n",
    "    except:\n",
    "        num_skipped = num_skipped + 1\n",
    "        continue\n",
    "print(f'Num skipped: {num_skipped}')\n",
    "game_level_dataset = pd.concat(all_games, ignore_index=True)\n",
    "print(f'shape of dataset: {game_level_dataset.shape}')\n",
    "game_level_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export out to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_level_dataset.to_csv('game-level-dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
