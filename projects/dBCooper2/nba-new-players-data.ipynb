{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Data :: New Player Dataset\n",
    "\n",
    "This notebook aims to create a new `players` dataset containing player statistics from 2004-2024. The old players dataset from `nba_api` was insufficient for our hypothesis tests, it was missing player positions, so we were unable to find correlations between positions and player statistics.\n",
    "\n",
    "The new dataset is pulled from the [nbaapi](<http://rest.nbaapi.com/index.html>) site, and after trying to get an automatic scraper to work, we settled on just downloading the data manually. The following functions will connect the downloaded json files and concatenate them into a pandas/polars DataFrame, and write the data to both CSV and PKL files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow\n",
    "import openpyxl\n",
    "import json\n",
    "import os\n",
    "\n",
    "data_dir = '/home/arch-db/Documents/github/bint-capstone/data-sources/player-responses/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Converting the JSON files to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_list_of_dfs(dir: str) -> list[pd.DataFrame]:\n",
    "    dfs = []\n",
    "\n",
    "    # fn = file name\n",
    "    for fn in os.listdir(dir):\n",
    "        if fn.endswith('.json'):\n",
    "            # fp = file path\n",
    "            fp = os.path.join(dir, fn)\n",
    "\n",
    "            # read json file, f = file\n",
    "            with open(fp, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "                df = pd.DataFrame(data)\n",
    "                dfs.append(df)\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = json_to_list_of_dfs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Concatenating the DataFrames and Writing to Files\n",
    "\n",
    "Now that the JSON data has been successfuly converted to DataFrames, we need to concatenate them and write to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_dfs(dfs:list[pd.DataFrame])->pd.DataFrame:\n",
    "    if dfs:\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_files(df, dir:str):\n",
    "    df.to_csv(f'{dir}new-player-data.csv')\n",
    "    df.to_pickle(f'{dir}new-player-data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_files(concat_dfs(dfs), data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
